{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 105 254 254\n",
      "  224  59  59   9   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 196 254 253 253\n",
      "  253 253 253 128   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  96 235 254 253 253\n",
      "  253 253 253 247 122   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   4 101 244 253 254 234 241\n",
      "  253 253 253 253 186  18   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  96 253 253 253 232  83 109\n",
      "  170 253 253 253 253 116   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 215 253 253 253 196   0   0\n",
      "   40 253 253 253 253 116   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8 141 247 253 253 237  29   0   0\n",
      "    6  38 171 253 253 116   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  13 146 253 253 253 253  57   0   0   0\n",
      "    0   0 156 253 253 116   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  40 253 253 253 253 178  27   0   0   0\n",
      "    0   0 156 253 253 116   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 136 204 253 253 253 192  27   0   0   0   0\n",
      "    0   0 156 253 253 116   0   0   0   0]\n",
      " [  0   0   0   0   0   0  28 195 254 254 254 250 135   0   0   0   0   0\n",
      "    0  99 255 254 254 117   0   0   0   0]\n",
      " [  0   0   0   0   0   0 118 253 253 253 253 142   0   0   0   0   0   0\n",
      "   19 170 253 253 216  62   0   0   0   0]\n",
      " [  0   0   0   0   0  42 212 253 253 253 253  38   0   0   0   0   0 124\n",
      "  188 253 253 253 174   0   0   0   0   0]\n",
      " [  0   0   0   0   0  59 253 253 253 237  93   3   0   0  31  40 130 247\n",
      "  253 253 253 204  13   0   0   0   0   0]\n",
      " [  0   0   0   0   0  59 253 253 253 154   0   0   0  54 218 254 253 253\n",
      "  253 253 253  38   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  59 253 253 253 215 156 156 156 209 253 255 253 253\n",
      "  253 192  97  15   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  55 242 253 253 253 253 253 253 253 253 254 253 253\n",
      "  204  23   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 118 253 253 253 253 253 253 253 253 254 216 174\n",
      "   13   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  54 116 243 253 253 253 253 253 146 117  62   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  53 132 253 253 192  57  13   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[399])\n",
    "print(x_train[399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense ,Flatten\n",
    "from tensorflow.keras.activations import relu,softmax\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(100,activation=relu,input_shape=(28,28)))\n",
    "model.add(Dense(10,activation=softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer =SGD(),loss =categorical_crossentropy,metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 28, 100)           2900      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 28, 10)            1010      \n",
      "=================================================================\n",
      "Total params: 3,910\n",
      "Trainable params: 3,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (60000, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-256362a91395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    715\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[1;32m    716\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m                          \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m                          \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                          \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (60000, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
